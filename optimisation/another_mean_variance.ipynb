{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numba\n",
    "from numba import jit,njit\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, the signature mean-variance optimization is implemented. And it is compared to the previous implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brief Description of the Optimization Process\n",
    "\n",
    "Aim: Given an expected signature `ES`, we want to find the mean-variance strategy described in [1] and `mean_variance.ipynb` in the same folder.\n",
    "\n",
    "Notation:\n",
    "\n",
    "We define the linear functional for weights of asset $i$ as $\\sum_{|w|\\leq n} a^i_{w} w$, where $n$ is the truncation level\n",
    "\n",
    "$\\prec$ is the right half shuffle product and $*$ represents the shuffle product. \n",
    "\n",
    "$A^i$ where $A$ is a set means the cartesian product of $A$ and itself for $i$ times. \n",
    "\n",
    "$p(\\cdot)$ is the function maps a word $a_1\\dots a_n$ to its position in a 1-d array returned by common signature computation functions (like `iisignautre` https://pypi.org/project/iisignature/ and `signatory` https://pypi.org/project/signatory/)\n",
    "\n",
    "By integration on $\\ell$, we mean the operation $\\ell\\prec \\mathbf{i}$ for some letter $\\mathbf{i}$.\n",
    "\n",
    "Step:\n",
    "\n",
    "1. Compute the abstract(symbolic) expression of $(w_1\\prec\\mathbf{i})*(w_2\\prec\\mathbf{j})$ and store the result in a dictionary with keyword the length of $w_1$ and $w_2$(Section 1);\n",
    "2. Compute the coefficents of $a^i_{w_1}a^j_{w_2}$ in the suquare of the integration. Initialize X2_ij=np.zeros(N,N), with $N=\\frac{d^m-1}{d-1}$.\n",
    "\n",
    "        for all w1,w2 in U_{i=0}^n {1,\\dots,d}^i{\n",
    "\n",
    "            indices = Plug w1,w2 into the dictionary of corresponding length obtained in 1.\n",
    "\n",
    "            for index in indices:\n",
    "            \n",
    "                X2_ij[p(w1),p(w2)] += ES[p(index)]\n",
    "            }\n",
    "\n",
    "3. Compute the coefficeints of $a^i_{w_1}$ in the integration by:\n",
    "    \n",
    "    Initialize R_i = np.zeros(N), with $N=\\frac{d^m-1}{d-1}$\n",
    "\n",
    "        for all w1,w2 in U_{i=0}^n {1,\\dots,d}^i{\n",
    "\n",
    "            indices = $w\\prec \\mathbf{i}$\n",
    "\n",
    "            for index in indices:\n",
    "        \n",
    "                R_i[p(w1),p(w2)] += ES[p(index)]\n",
    "                }\n",
    "            \n",
    "4. Get the total variance of the portofolio w.r.t a set of coefficients $\\{c_i|i=1,..,n\\}$ where $c_i=\\{a^i_w|w\\in\\bigcup_{i=0}^n \\{1,\\dots,d\\}^{i}\\}$ as:\n",
    "\n",
    "        result = 0\n",
    "\n",
    "        for i,j in range(dim):\n",
    "\n",
    "            result += c_i^T X2_ij c_j^T - (R_i c_i) * (R_j c_j)\n",
    "\n",
    "\n",
    "\n",
    "By concatenate $\\{c_i|i=1,..,n\\}$ into a single array `c`, we would have the better form:\n",
    "\n",
    "\\begin{align*}\n",
    "    \\min_{c\\in \\mathbb{R}^{dN}} \\ \\ & c^T \\Sigma_{sig} c \\\\\n",
    "        &Ac = b\\\\\n",
    "        &Bc > 0_d\n",
    "\\end{align*}\n",
    "\n",
    "where $\\Sigma_{sig} = \\begin{pmatrix}\n",
    "    X2\\_11 & X2\\_12 &\\dots &X2\\_1d\\\\\n",
    "    \\vdots & \\vdots &\\dots &X2\\_2d\\\\\n",
    "    X2\\_d1 & X2\\_d2 &\\dots &X2\\_dd\\\\\n",
    "\\end{pmatrix}-RR^T$ with R=[R_1,...,R_d],\n",
    "\n",
    "$A =  \\begin{pmatrix}\n",
    "    ES& ES &\\stackrel{\\text{d times}}{\\dots} &ES\\\\\n",
    "    R\\_1 &R\\_2 &\\dots &R\\_d\n",
    "\\end{pmatrix}$, $b = [1,r]^T$ with $r$ the expected return.\n",
    "\n",
    "$B = \\begin{pmatrix}\n",
    "    ES & 0 &\\stackrel{\\text{d-1 times}}{\\dots} &0\\\\\n",
    "    0 & ES &\\stackrel{\\text{d-2 times}}{\\dots} &0\\\\\n",
    "    \\vdots &\\dots & \\ddots  &0\\\\\n",
    "    0 &\\dots  &\\stackrel{\\text{d-1 times}}{\\dots} &ES\n",
    "\\end{pmatrix}$\n",
    "\n",
    "This coincides with the form introuced in [1].\n",
    "\n",
    "Finally, we solve the optimization problem and get the optimal set of coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Advantages of the new implementation:\n",
    "\n",
    "1. The code is much faster compared to the previous version (more than 600 times faster for paths with 3 channels truncated at level 3! And the difference increases rapidly when truncated level and number of channels increases)\n",
    "\n",
    "2. Can benefit from parallelization, vectorization and accerlation on GPU. \n",
    "\n",
    "3. It is also intutive since every coefficents of each $a^i_w$ is extracted and listed in an arrary with a clear index corresponding, so every linear functionals could be written as matrix multiplications\n",
    "\n",
    "4. It is also considered to be general enough and space is left for further extensions (although less straightforward than the old one)\n",
    "\n",
    "5. The code is much cleaner after expected signatures are computed.\n",
    "\n",
    "\n",
    "\n",
    "#### Disadvantages of the new implementation:\n",
    "1. The operations on the words are very intuitive and may need to redefine a function for substitution for every different operations. So not very user-friendly\n",
    "\n",
    "2. Defining new operations is not as straightforward as the previous implementation does (could be solved when a class of operations are defined)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Some Useful Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit([numba.int32(numba.int32,numba.int32)])\n",
    "def length_of_signature(dim,level):\n",
    "    \"\"\"Length of signature of paths with `dim` channels at level `level`.\"\"\"\n",
    "    return (dim**level-1)//(dim-1)\n",
    "\n",
    "@njit([numba.int32(numba.int32[:],numba.int32)])\n",
    "def convert_indices2_position_in_signature(indices,dim):\n",
    "    \"\"\" Given a word [a_1,a_2,...,a_n], get its position in a 1-d array returned by \n",
    "        common signature computation functions(like iisignautre[1] and signatory[2]).\n",
    "        [1] https://pypi.org/project/iisignature/\n",
    "        [2] https://pypi.org/project/signatory/\n",
    "    \"\"\"\n",
    "    if len(indices)==0:\n",
    "        return 0\n",
    "    n = len(indices)\n",
    "    index = 0\n",
    "    # position at the level the indices lie in\n",
    "    for i in range(n):\n",
    "        index += indices[i]*dim**(n-i-1)\n",
    "    level = len(indices)\n",
    "    # add the number of elements in previous levels\n",
    "    index += length_of_signature(dim,level)\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Implementation of Shuffle and Half Shuffle Products. Could be replaced by other implementations.\"\"\"\n",
    "import copy\n",
    "class Tree:\n",
    "    def __init__(self, data):\n",
    "        self.children = []\n",
    "        self.data = data\n",
    "        self.list = [[],[]]\n",
    "        self.number = 2\n",
    "\n",
    "def _get_paths(t, paths=None, current_path=None):\n",
    "    if paths is None:\n",
    "        paths = []\n",
    "    if current_path is None:\n",
    "        current_path = []\n",
    "\n",
    "    current_path.append(t.data)\n",
    "    if len(t.children) == 0:\n",
    "        paths.append(current_path)\n",
    "    else:\n",
    "        for child in t.children:\n",
    "            _get_paths(child, paths, list(current_path))\n",
    "    return paths\n",
    "\n",
    "def half_shuffle(x):\n",
    "    \"\"\" Input: an iterable objects of lists l1,l2,..,ln\n",
    "        Return: l1 \\prec (l2 \\prec (ln-1 \\prec ln))))))\"\"\"\n",
    "    for i in range(len(x)):\n",
    "        x[i] = x[i][::-1]\n",
    "    x0 = x[0].pop()\n",
    "    root = Tree(x0)\n",
    "    root.list = x\n",
    "    # \n",
    "    stack = [root]\n",
    "    while stack:\n",
    "        current_node = stack.pop()\n",
    "        for n in range(current_node.number):\n",
    "            if current_node.list[n]:\n",
    "                node_list = copy.deepcopy(current_node.list)\n",
    "                node = Tree(node_list[n].pop())\n",
    "                node.number = max([n+2,current_node.number])\n",
    "                node.number = min([node.number,len(current_node.list)])\n",
    "                node.list = node_list\n",
    "                current_node.children.append(node)\n",
    "        for node in current_node.children:\n",
    "            stack.append(node)\n",
    "\n",
    "    paths = _get_paths(root)\n",
    "\n",
    "    return paths\n",
    "    \n",
    "def shuffle(x):\n",
    "    \"\"\" Input: an iterable objects of lists l1,l2,..,ln\n",
    "        Return l1*l2*...*ln\n",
    "    \"\"\"\n",
    "    for i in range(len(x)):\n",
    "        x[i] = x[i][::-1]\n",
    "    paths = []\n",
    "    for i in range(len(x)):\n",
    "        x_c = copy.deepcopy(x)\n",
    "        x0 = x_c[i].pop()\n",
    "        root = Tree(x0)\n",
    "        root.list = x_c\n",
    "\n",
    "        stack = [root]\n",
    "        while stack:\n",
    "            current_node = stack.pop()\n",
    "            for n in range(len(x)):\n",
    "                if current_node.list[n]:\n",
    "                    node_list = copy.deepcopy(current_node.list)\n",
    "                    node = Tree(node_list[n].pop())\n",
    "                    node.list = node_list\n",
    "                    current_node.children.append(node)\n",
    "            for node in current_node.children:\n",
    "                stack.append(node)\n",
    "\n",
    "        paths += _get_paths(root)\n",
    "\n",
    "    return paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Generate cartesian product of a numpy array with repeats.\n",
    "    The code is from Hadrien Titeux in stackoverflow \n",
    "    https://stackoverflow.com/questions/57128975/cartesian-product-in-numba. \n",
    "    It can be replaced by itertools.poroduct\"\"\"\n",
    "\n",
    "@njit(numba.int32[:,:](numba.int32[:]))\n",
    "def cproduct_idx(sizes: np.ndarray):\n",
    "    \"\"\"Generates ids tuples for a cartesian product\"\"\"\n",
    "    assert len(sizes) >= 2\n",
    "    tuples_count  = np.prod(sizes)\n",
    "    tuples = np.zeros((tuples_count, len(sizes)), dtype=np.int32)\n",
    "    tuple_idx = 0\n",
    "    # stores the current combination\n",
    "    current_tuple = np.zeros(len(sizes))\n",
    "    while tuple_idx < tuples_count:\n",
    "        tuples[tuple_idx] = current_tuple\n",
    "        current_tuple[0] += 1\n",
    "        # using a condition here instead of including this in the inner loop\n",
    "        # to gain a bit of speed: this is going to be tested each iteration,\n",
    "        # and starting a loop to have it end right away is a bit silly\n",
    "        if current_tuple[0] == sizes[0]:\n",
    "            # the reset to 0 and subsequent increment amount to carrying\n",
    "            # the number to the higher \"power\"\n",
    "            current_tuple[0] = 0\n",
    "            current_tuple[1] += 1\n",
    "            for i in range(1, len(sizes) - 1):\n",
    "                if current_tuple[i] == sizes[i]:\n",
    "                    # same as before, but in a loop, since this is going\n",
    "                    # to get run less often\n",
    "                    current_tuple[i + 1] += 1\n",
    "                    current_tuple[i] = 0\n",
    "                else:\n",
    "                    break\n",
    "        tuple_idx += 1\n",
    "    return tuples\n",
    "\n",
    "@njit\n",
    "def cartesian_product(*arrays):\n",
    "    sizes = [len(a) for a in arrays]\n",
    "    sizes = np.asarray(sizes, dtype=np.int8)\n",
    "    tuples_count  = np.prod(sizes)\n",
    "    array_ids = cproduct_idx(sizes)\n",
    "    tuples = np.zeros((tuples_count, len(sizes)))\n",
    "    for i in range(len(arrays)):\n",
    "        tuples[:, i] = arrays[i][array_ids[:, i]]\n",
    "    return tuples\n",
    "\n",
    "@njit\n",
    "def cartesian_product_repeat(array, repeat):\n",
    "    sizes = [len(array) for _ in range(repeat)]\n",
    "    sizes = np.asarray(sizes, dtype=np.int32)\n",
    "    tuples_count  = np.prod(sizes)\n",
    "    array_ids = cproduct_idx(sizes)\n",
    "    tuples = np.zeros((tuples_count, len(sizes)),dtype=np.int32)\n",
    "    for i in range(repeat):\n",
    "        tuples[:, i] = array[array_ids[:, i]]\n",
    "    return tuples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Compute Operations on Words\n",
    "\n",
    "The idea of implementing operations on words (like concatenation by another word, shuffle product, half shuffle product and composition of these) is to first perform the operation abstractly(symbolically) and then plug the concrete words into the symbolically result. \n",
    "\n",
    "For example, to compute an operation $O$ on two words $w_1,w_2$, we first compute $O(11\\stackrel{\\text{m times}}{\\cdots}1,22\\stackrel{\\text{n times}}{\\cdots}2)$, where $m,n$ is the length of the words as operands. Then, we replace 1's in all result words by letters in $w_1$ and replace 2's by letters in $w_2$ in order. This would give $O(w_1,w_2)$. This could be extend to operations on multiple words, by computing $O(\\{k\\stackrel{|w_k|\\text{ times}}{\\cdots}k\\}_{k=1,\\dots,m})$ and replace $k$'s by letters in $w_k$ in order. \n",
    "\n",
    "Note the bioperation may include some operations with other words. For simple operations, like $O_{ij}=(w_1 \\prec i)*(w_2 \\prec j)$ where $\\mathbf{i},\\mathbf{j}$ are two letters. We may simply concate $-1$ to $11\\stackrel{\\text{m times}}{\\cdots}1$ and $-2$ to $22\\stackrel{\\text{m times}}{\\cdots}2$, and then compute their shuffle product. After that, just replace 1 by letter in $w_1$ and 2 by $w_2$ as before, but additionally replace -1 by $\\mathbf{i}$ and -2 by $\\mathbf{j}$.\n",
    "\n",
    "We could also use more complicated structure to specify the external words(Not recommand!). For example, let $O_{* w_3,w_4}(w_1,w_2):= (w_1 * w_3) * (w_2 * w_4)$. We may use $(-1,(w_3)_1),(-1,(w_3)_2),\\dots$ to represent $w_3$ and $(-2,(w_4)_1),(-2,(w_4)_2),\\dots$ for $w_4$. A better way is to treat them as multi-input operands: $O(w_1,w_2,w_3,w_4)$. \n",
    "\n",
    "Thus, it is believed that most word operations could be achieved in these ways."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extensions:\n",
    "\n",
    "A big advantage of this method is that the input and output are both nd_arrays, and the shape of output arrays is fixed (we may know it simply from the abstract example pre-comouted). We may make it vectorised. Hence, in our later example, we may send $0000, 0001, ..., 2222$ and get an array of all the results of their concatenated shuffle products! It is hard to achieve this in `numba`, but may be easier in `jax` by setting static argumnets with `@partial`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Here defines the used word operations: shuffle product, and first cancatenate a word representing \n",
    "    integration along one of the member path of the process, and then take shuffle product with another\n",
    "    integrated paths.\"\"\"\n",
    "\n",
    "# shuffle product\n",
    "def word_shuffle_product(level1,level2):\n",
    "    \"\"\" Return a dictionary with '{i}{j}' as the keyword, where i, j are the length\n",
    "        of the two word-operands, and is from 0 to `level1`\\`level2` respectively. \n",
    "    \"\"\"\n",
    "    word_shuffle_dict = numba.typed.Dict.empty(key_type=numba.types.unicode_type,value_type=numba.types.int32[:,:])\n",
    "    for i in range(1,level1):\n",
    "        for j in range(1,level2):\n",
    "            word_shuffle_dict[f'{i}{j}'] = np.array(shuffle([[1 for _ in range(i)],[2 for _ in range(j)]]),dtype=np.int32)\n",
    "    for i in range(level1):\n",
    "        word_shuffle_dict[f'{i}0'] = np.ones((1,i),dtype=np.int32)\n",
    "    for i in range(1,level2):\n",
    "        word_shuffle_dict[f'0{i}'] = np.ones((1,i),dtype=np.int32)\n",
    "    return word_shuffle_dict\n",
    "\n",
    "# integrate(concatenate a letter) and shuffle\n",
    "def word_concatenate_shuffle(level1,level2):\n",
    "    \"\"\" Return a dictionary with '{i}{j}' as the keyword, where i, j are the length\n",
    "        of the two word-operands, and is from 0 to `level1`\\`level2` respectively.\n",
    "        The value of 'ij' should be the output of 1...1(i times)[-1] \\shuffle 2...2(j times)[-2].\n",
    "    \"\"\"\n",
    "    word_concatenate_shuffle_dict = numba.typed.Dict.empty(key_type=numba.types.unicode_type,value_type=numba.types.int32[:,:])\n",
    "    for i in range(level1):\n",
    "        for j in range(level2):\n",
    "            # shuffle product between 1...1[-1] and 2...2[-2] \n",
    "            word_concatenate_shuffle_dict[f'{i}{j}'] = np.array(shuffle([[1 for _ in range(i)]+[-1],[2 for _ in range(j)]+[-2]]),dtype=np.int32)\n",
    "    return word_concatenate_shuffle_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit(nopython=True)\n",
    "#@vectorize([numba.float64(numba.int32[:,:],numba.float64[:],numba.int32[:],numba.int32,numba.int32)], target='parallel')\n",
    "def apply_bioperation_to_word(x,signature,word_operation,dim,level,*o):\n",
    "    \"\"\"Given a word, return the position in the signature according to a pre-computed abstract word operation,\n",
    "        I didn't see the way to generalize it, so user may require to define it every time for different operations.\n",
    "        (like this is in fact defined for `word_concatenate_shuffle`)\n",
    "        But it may be generalized in some way, although it is not necessary to do that(could be taken place by multiple inputs operation\n",
    "        The generalization to multiple inputs is straightfoward!\n",
    "\n",
    "        Arguments:\n",
    "        x: an array of shape (level[0]+level[1],). Concatenated by word1 and word2\n",
    "        signature: an array of shape (N,) N=(dim^m-1)(dim-1) the expected signature\n",
    "        word_operation: an array obtained from functions like `word_concatenate_shuffle`[level1,level2].\n",
    "            It contains  1's,2's,.. and other stuff (like -1's,-2's,...)\n",
    "        dim: int dimension of the paths\n",
    "        level: a tuple (level1,level2), the level of two words\n",
    "        \"\"\"\n",
    "    result = 0\n",
    "    # get the position of the word in signature\n",
    "    i1 = convert_indices2_position_in_signature(x[:level[0]],dim)\n",
    "    i2 = convert_indices2_position_in_signature(x[level[0]:],dim)\n",
    "\n",
    "    for k in range(len(word_operation)):\n",
    "        xx = word_operation[k]\n",
    "        \n",
    "        indices = np.zeros(len(xx),dtype=np.int32)\n",
    "        indices[np.where(xx==1)] = x[:level[0]] # replace 1 by letters in w1\n",
    "        indices[np.where(xx==2)] = x[level[0]:] # replace 2 by letters in w2\n",
    "        indices[np.where(xx==-1)] = o[0] # replace -1 by m\n",
    "        indices[np.where(xx==-2)] = o[1] # replace -2 by n\n",
    "\n",
    "        index = convert_indices2_position_in_signature(indices,dim)\n",
    "        #print(convert_indices2_position_in_signature(indices,dim))\n",
    "        result += signature[index]\n",
    "    return ((i1,i2),result)\n",
    "\n",
    "@njit\n",
    "def concatenate_a_letter_to_words(x,signature,dim,m):\n",
    "    \"\"\"Given a word, return the position of the word in signature and the corresponding coefficients\"\"\"\n",
    "    i = convert_indices2_position_in_signature(x,dim) # get the position of the word in signature\n",
    "    indices = np.concatenate((x,np.array([m],dtype=np.int32))) # get the indices\n",
    "\n",
    "    # read the elements corresponding to the indices in the signature\n",
    "    result = signature[convert_indices2_position_in_signature(indices,dim)] \n",
    "    return (i,result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit(nopython=True,parallel=True)\n",
    "def squared_integration_functional(signature,word_operation_dict,dim,level,m,n):\n",
    "    \"\"\" Return the coefficeints matrix of a^m_p(w)a^n_p(w) for w in W^d_m(A), in \n",
    "        (l_m \\prec m) * (l_n \\prec n) where  W^d_m(A) is the set of words of \n",
    "        dimension d up to length m. \n",
    "        \n",
    "        Arguments:\n",
    "        -----------------------------\n",
    "        word_operation_dict: dict   a dictionary containing expression of w_i, w_j with keywords 'ij'\n",
    "        m,n: int    the specific channel \n",
    "        \"\"\"\n",
    "    sig_len = length_of_signature(dim,level) # length of signature at (level, dim)\n",
    "    weights = np.zeros((sig_len,sig_len)) # (l_m < m) * (l_n < n) = sum weights_ij (a^m_(w_1) a^n_(w_2))\n",
    "    for i in range(level):\n",
    "        for j in range(level):\n",
    "            if i+j ==0:\n",
    "                # a^m_0 a^n_0 (m*n)\n",
    "                ii1 = convert_indices2_position_in_signature(np.array([m,n],dtype=np.int32),dim) \n",
    "                ii2 = convert_indices2_position_in_signature(np.array([n,m],dtype=np.int32),dim)\n",
    "                weights[0,0] = signature[ii1]+signature[ii2]\n",
    "                continue\n",
    "            elif i+j == 1:\n",
    "                generator = np.array([p for p in range(dim)],dtype=np.int32).reshape(dim,1)\n",
    "            else:\n",
    "                generator = cartesian_product_repeat(np.arange(dim),i+j)\n",
    "            for k in range(len(generator)):\n",
    "                x = generator[k]\n",
    "                # a^m_x1 a^n_x2 (x1 * x2), ii = (p(x1),p(x2), es = sum S()_(x1 * x2)\n",
    "                ii, es = apply_bioperation_to_word(x,signature,word_operation_dict[f'{i}{j}'],dim,(i,j),m,n)\n",
    "\n",
    "                weights[ii[0],ii[1]] = es\n",
    "    return weights\n",
    "\n",
    "def get_sig_variance(signature,word_operation_dict,dim,level):\n",
    "    \"\"\" Get the coefficeints of total variance by block maztrixs (X2_ij)_{1<= i,j <= d}\n",
    "        Hence the total square of the return is a.T@X2@a, where a_p(w) is the collection \n",
    "        of coefficeints of w.\"\"\"\n",
    "    sig_len = length_of_signature(dim,level)\n",
    "    weights = np.zeros((sig_len*dim,sig_len*dim))\n",
    "    for m in range(dim):\n",
    "        for n in range(dim):\n",
    "            weights[m*sig_len:(m+1)*sig_len,n*sig_len:(n+1)*sig_len] = squared_integration_functional(signature,word_operation_dict,dim,level,m,n)\n",
    "    return weights\n",
    "        \n",
    "\n",
    "@jit(nopython=True,parallel=True)\n",
    "def integration_functional_coeff(signature,dim,level,m):\n",
    "    \"\"\" Return the coefficeints vectors R_m of a^m_p(w) for w in W^d_m(A) in expected returns. \n",
    "        Hence E[l_m(S(X))] = R_m @ a, where a_p(w) is the collection of coefficeints of w\"\"\"\n",
    "    coeff = np.zeros(length_of_signature(dim,level))\n",
    "\n",
    "    coeff[0] = signature[m+1]   # when j=0, the coefficient of a_0 is ES(x)^m\n",
    "\n",
    "    for j in range(1,level):\n",
    "        if j == 1:\n",
    "            generator = np.array([p for p in range(dim)],dtype=np.int32).reshape(dim,1)\n",
    "        else:\n",
    "            generator = cartesian_product_repeat(np.arange(dim),j)\n",
    "        for k in range(len(generator)):\n",
    "            # a^m_w w \\prec m. Hence the coefficents is ES(X)^wm\n",
    "            i,es = concatenate_a_letter_to_words(generator[k],signature,dim,m)\n",
    "            coeff[i] += es\n",
    "    return coeff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weights_sum_coeff(signature,dim,level):\n",
    "    \"\"\" Get coefficeints of a^m_w in the sum of weights of all assets as WS.\n",
    "        sum w_i = WS @ a, where a^i_p(w) is the collection of coefficeints of w\n",
    "        on the i'th channel\"\"\"\n",
    "    sig_len = length_of_signature(dim,level)\n",
    "    truncated_signature = signature[:sig_len]\n",
    "    coeff =  np.tile(truncated_signature,dim)\n",
    "    return coeff\n",
    "\n",
    "@njit\n",
    "def get_weights_coeff(signature,m,dim,level):\n",
    "    \"\"\" Get coefficeints of a^m_w in the sum of weights of all assets as WS.\n",
    "        w_i = W_i @ a, where a^i_p(w) is the collection of coefficeints of w\n",
    "        on the i'th channel\"\"\"\n",
    "    sig_len = length_of_signature(dim,level)\n",
    "    coeff = np.zeros(sig_len*dim)\n",
    "    coeff[m*sig_len:(m+1)*sig_len] = signature[:sig_len]\n",
    "    return coeff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 3. Defining Minimization Problems\n",
    "\n",
    "Now we perform an example for the dataset in the period '2017-04-01' to '2017-06-01' at level 2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yousi\\AppData\\Local\\Temp\\ipykernel_22632\\3130669118.py:8: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df.index = pd.to_datetime(df.index)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "N = 3\n",
    "\n",
    "# stocks\n",
    "df      = pd.read_csv('stocks.csv', index_col=0)\n",
    "names   = df.columns[:N].to_list()\n",
    "df.index = pd.to_datetime(df.index)\n",
    "\n",
    "df2 = df[names].loc[(df.index > pd.Timestamp('2017-04-01')) & (df.index < pd.Timestamp('2017-06-01'))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the loss converges after 1000 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mogptk'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmogptk\u001b[39;00m  \u001b[38;5;66;03m# pytorch >= 11.0 is required\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;66;03m# pytorch 11.0 is used\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msignatory\u001b[39;00m \u001b[38;5;66;03m# if cannot find a version compatible with pytorch, \u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'mogptk'"
     ]
    }
   ],
   "source": [
    "import mogptk  # pytorch >= 11.0 is required\n",
    "import torch # pytorch 11.0 is used\n",
    "import signatory # if cannot find a version compatible with pytorch, \n",
    "                 # install locally follows insturcction on https://github.com/patrick-kidger/signatory\n",
    "\n",
    "mogptk.use_cpu()\n",
    "\n",
    "#if torch.cuda.is_available():\n",
    "#    mogptk.use_gpu(0)\n",
    "\n",
    "model_life = 5\n",
    "training_size = 0.95\n",
    "Q = 3\n",
    "init_method = 'BNSE'\n",
    "method = 'Adam'\n",
    "\n",
    "df2.loc[:,'time'] = np.linspace(0,1,len(df2)+model_life)[:len(df2)]\n",
    "gpm_dataset = mogptk.LoadDataFrame(df2, x_col='time', y_col=names)\n",
    "for channel in gpm_dataset:\n",
    "    # normalize the path and fit the normalized path by y=a*t+b_t. The train would be performed on b_t \n",
    "    channel.transform(mogptk.TransformNormalize())\n",
    "    channel.transform(mogptk.TransformDetrend())\n",
    "for name in names:\n",
    "    # remove the data after `training_size` as test data \n",
    "    gpm_dataset[name].remove_randomly(pct=1-training_size)\n",
    "\n",
    "# set the model and train it\n",
    "gpm = mogptk.SM_LMC(gpm_dataset, Q=Q,inference=mogptk.Exact())\n",
    "gpm.init_parameters(init_method)\n",
    "loss,error = gpm.train(method=method, lr=0.01, iters=1000, verbose=False,plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that here an additional compile time is included. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "dim = 3\n",
    "level = 3\n",
    "word_concatenate_shuffle_dict = word_concatenate_shuffle(level,level)\n",
    "\n",
    "# generate a list containing paths of each channels\n",
    "X = gpm.sample(np.linspace(0,1,101),n=1000)\n",
    "# rearrange the result into (number_of_paths,time_step+1,dim)\n",
    "paths = np.concatenate([x.T.reshape(-1,100+1,1) for x in X],axis=2)\n",
    "# normalize with the mean of the price at time 0\n",
    "paths/= np.mean(paths[:,0],axis=0)\n",
    "\n",
    "tensor_path = torch.tensor(paths,device='cuda:0')\n",
    "# compute expected signature\n",
    "signature = signatory.signature(tensor_path,level*2).mean(axis=0)\n",
    "# add zero level to the signature\n",
    "signature = np.concatenate([[1],np.array(signature.cpu())])\n",
    "\n",
    "start0 = time.time()\n",
    "# get coefficients of a^i_w in mean and variance functions \n",
    "mean_weights = np.array([integration_functional_coeff(signature,dim,level,i) for i in range(dim)]).reshape(1,-1)\n",
    "var_coeff = get_sig_variance(signature,word_concatenate_shuffle_dict,dim,level)\n",
    "\n",
    "# get coefficients of a^i_w in weights and weight sum functions \n",
    "weights_sum = get_weights_sum_coeff(signature,dim,level)\n",
    "A = np.array([get_weights_coeff(signature,i,dim,level) for i in range(dim)])\n",
    "\n",
    "print('time for getting coefficents(including compiling time):',time.time()-start0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize,LinearConstraint\n",
    "\n",
    "exp_return = 0.05\n",
    "\n",
    "object_function = lambda coeff: coeff.T@(var_coeff+np.eye(var_coeff.shape[0])*1e-5)@coeff\n",
    "\n",
    "cons = ({'type': 'eq', 'fun': lambda coeff: np.squeeze(mean_weights)@coeff-exp_return},\n",
    "                   {'type': 'eq', 'fun': lambda coeff: weights_sum@coeff-1},\n",
    "                   LinearConstraint(A,lb=np.zeros(dim),ub=np.ones(dim)))\n",
    "\n",
    "start = time.time()\n",
    "res = minimize(object_function, np.ones(length_of_signature(dim,level)*3), method='SLSQP',\n",
    "               constraints=cons)\n",
    "print('time for optimisation:',time.time()-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a look at the optimised weight functional. Looks good! All weights are positive and the expected return is very close to our preset expected return."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('weights of each asset:',A@res.x) \n",
    "print('result portfolio variance:',res.x.T@var_coeff@res.x)\n",
    "print('result portfolio expected return:',np.squeeze(mean_weights)@res.x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 4. Comparision of Previous Optimisation\n",
    "\n",
    "A first important thing is to check whether the linear functionals obtained from two implements agrees. Then, the time for generating the linear functionals is recorded and compared. The signatures are both computed by `signatory.signature` on GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from src.optimisation.signature import ES\n",
    "from src.signature_trading import _get_funcs\n",
    "\n",
    "class RandomPathExpectedSignautre():\n",
    "    model_life = np.infty\n",
    "    def _get_paths(self,n,time_step,channel):\n",
    "        self.paths = torch.rand(n,time_step+1,channel)\n",
    "    \n",
    "    def ExpectedSignature(self,level,transformation=None):\n",
    "        return ES(self.paths,level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "import signatory\n",
    "import time\n",
    "\n",
    "word_concatenate_shuffle_dict = word_concatenate_shuffle(4,4)\n",
    "class Test(unittest.TestCase):\n",
    "    def __init__(self,dim,level):\n",
    "        # get a set of random path and compute its expected signature\n",
    "        res = RandomPathExpectedSignautre()\n",
    "        res._get_paths(1000,100,dim)\n",
    "\n",
    "        # generator functionals for old implementation\n",
    "        start1 = time.time()\n",
    "        get_funcs = _get_funcs(dim,level,res.ExpectedSignature)\n",
    "        self.funcs = get_funcs.funcs\n",
    "        print(f\"time used for constructing linear functionals on paths with {dim} channels, trucated at level {level} with old implement:\", \n",
    "                time.time()-start1)\n",
    "        # generate coefficients for new implementation\n",
    "        start2 = time.time()\n",
    "\n",
    "        paths = res.paths.clone().detach().to('cuda:0')\n",
    "        signature = signatory.signature(paths,(level+1)*2).mean(axis=0)\n",
    "        signature = np.concatenate([[1],np.array(signature.cpu())])\n",
    "\n",
    "        mean_weights = np.array([integration_functional_coeff(signature,dim,level+1,i) for i in range(dim)]).reshape(1,-1)\n",
    "        var_coeff = get_sig_variance(signature,word_concatenate_shuffle_dict,dim,level+1)\n",
    "\n",
    "        weights_sum = get_weights_sum_coeff(signature,dim,level+1)\n",
    "        weights = np.array([get_weights_coeff(signature,i,dim,level+1) for i in range(dim)])\n",
    "\n",
    "        self.funcs2 = [lambda coeff: coeff.reshape(-1,).T@(var_coeff-mean_weights.T@mean_weights)@coeff.reshape(-1,)]\n",
    "        self.funcs2 += [lambda coeff: np.squeeze(mean_weights)@coeff.reshape(-1,),lambda coeff: weights_sum@coeff.reshape(-1,)]\n",
    "        self.funcs2 += [lambda coeff: weights@coeff.reshape(-1,)]\n",
    "        print(f\"time used for constructing linear functionals on paths with {dim} channels, trucated at level {level} with new implement:\", \n",
    "                time.time()-start2)\n",
    "\n",
    "        self.coeff = np.random.random((dim,(dim**(level+1)-1)//(dim-1)))\n",
    "\n",
    "    def test_variance(self):\n",
    "        var1 = self.funcs[0](self.coeff)\n",
    "        var2 = self.funcs2[0](self.coeff)\n",
    "        self.assertAlmostEqual(var1,var2,8)\n",
    "\n",
    "    def test_mean(self):\n",
    "        mean1 = self.funcs[1](self.coeff)\n",
    "        mean2 = self.funcs2[1](self.coeff)\n",
    "        self.assertAlmostEqual(mean1,mean2,8)\n",
    "\n",
    "    def test_weights_sum(self):\n",
    "        weights_sum1 = self.funcs[2](self.coeff)\n",
    "        weights_sum2 = self.funcs2[2](self.coeff)\n",
    "        self.assertAlmostEqual(weights_sum1,weights_sum2,8)\n",
    "\n",
    "    def test_weights(self):\n",
    "        weights1 = self.funcs[3](self.coeff)\n",
    "        weights2 = self.funcs2[3](self.coeff)\n",
    "        for i,j in zip(weights1,weights2):\n",
    "            self.assertAlmostEqual(i,j,8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the new implementation is much faster than the old one (more than 300 times for 4-channel paths at truncated level 2 and almost 600 times for channel 3 level 3) with an error less than 1e-8. And we can see that the time used increases more rapidly when the truncated level and number of channel increases. Note that the functions are already compiled in jit, but this could always be achieved by pre compiling the code when loading the package, and the compile time is not too long in this case(no longer than 10s according to result in section 3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dim,level in [(3,1),(3,2),(3,3),(4,2)]:\n",
    "    test = Test(dim,level)\n",
    "    test.test_variance()\n",
    "    test.test_mean()\n",
    "    test.test_weights_sum()\n",
    "    test.test_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference:\n",
    "\n",
    "[1] Owen Futter, Blanka Horvath, and Magnus Wiese. Signature trading: A path-dependent\n",
    "extension of the mean-variance framework with exogenous signals. Available at SSRN 4541830,\n",
    "2023"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
